.. image:: https://img.shields.io/badge/python-3.x-blue.svg
   :target: https://www.python.org/downloads/
.. image:: https://img.shields.io/badge/License-MIT-yellow.svg
   :target: ./LICENSE

=============================================================
Changing Tire Assistant â€“ Computer Vision & NLP Project ğŸš—
=============================================================

**ğŸš§ Project Status: Under Development ğŸš§**

---

Overview ğŸŒŸ
=============
This assistant leverages real-time computer vision and NLP to guide users through changing a flat tire using an egocentric (chest-mounted) camera. 

The assistant detects tools, tracks task progress, and provides interactive, step-by-step visual and voice instructions for changing a vehicle tire.

---

System Features ğŸ› ï¸
=====================
- **Real-Time Tool Detection**:
  - Identifies car jack, wheel wrench, etc.
- **Action Recognition**:
  - Tracks task progression like loosening nuts, jacking the car, replacing the wheel, etc.
- **Voice Assistant**:
  - Responds to user queries such as "What's next?"
- **Edge-Friendly Pipeline**:
  - Designed for deployment on mobile or embedded systems with minimal latency.

---

Project Structure ğŸ“
=======================
.. code-block:: text

    â”œâ”€â”€ action_recognition/             
    â”œâ”€â”€ Object Detection/             
    â”œâ”€â”€ UML Model             

---

Models Used ğŸ§ 
=================
- **Object Detection**: YOLOv11n fine-tuned on tire-change-specific tools and components
- **Action Recognition**: We are trying different models SlowFast , TSM , TimeDistributed EfficientNetB0
- **Voice Assistant**: Whisper-based STT with a custom NLP pipeline for contextual understanding

---

Data ğŸ“Š
========
We collected and curated a custom dataset specifically for the tire change domain:

Data Collection Methodology ğŸ¥
----------------------------------------
- **Primary Source**: Self-collected footage changing two tires on a Renault Megane 2, recorded with Samsung A50 smartphones from chest-mounted positions
- **Secondary Source**: Curated YouTube videos showing different tire change scenarios and vehicle types
- **Other Source**: We scraped the web to gather tool specifications and case studies related to flat tires. Additionally, we recorded standard videos and extracted frames for further processing. These frames were then manually annotated to identify tools, resulting in a dataset of approximately 1,597 annotated images.
- **Annotation Process**: Manual annotation of action segments and tool detection bounding boxes

Dataset Structure ğŸ—ƒï¸
------------------------------
.. code-block:: text

    data/
    â”œâ”€â”€ lower_car/              # Videos/frames of lowering the car from the jack
    â”œâ”€â”€ lift_car_with_jack/     # Videos/frames of raising the car with jack
    â”œâ”€â”€ tighten_bolts/          # Videos/frames of final bolt tightening with wrench
    â”œâ”€â”€ initial_wrench_tighten/# Videos/frames of initial wrench positioning
    â”œâ”€â”€ place_spare_tire/       # Videos/frames of positioning the spare tire
    â”œâ”€â”€ remove_tire/            # Videos/frames of removing the flat tire
    â”œâ”€â”€ hand_tighten_bolts/     # Videos/frames of hand-tightening bolts
    â”œâ”€â”€ loosen_bolts/           # Videos/frames of loosening wheel bolts
    â”œâ”€â”€ remove_bolts/           # Videos/frames of removing wheel bolts
    â”œâ”€â”€ labels.csv              # Action timestamps and class annotations
    â””â”€â”€ README.txt              # Dataset documentation

https://github.com/user-attachments/assets/6d6bef7f-5d31-4b78-b57b-93b3566c5007

Changing Tire Assistant - Computer Vision & NLP Project: Models Testing ğŸ§ª
============================================================================
This document outlines the setup and usage of the models implemented within the Changing Tire Assistant project.

---

Object Detection ğŸ¯
=====================
This section details the implementation of the object detection model.

YOLOv11 Nano Implementation Guide ğŸ“„
----------------------------------------------
**Description**: This model is utilized for identifying various objects relevant to the tire-changing process.

Prerequisites âœ…
--------------------------
* Python 3.1X
* Ultralytics package (version compatible with your GPU, you can ask any AI for that)
* CUDA 11.8+ (recommended for GPU acceleration)

---

Setup âš™ï¸
-------------
You can set up the project by cloning the full repository or by cloning only the `Object Detection` directory using Git's sparse checkout feature.

Option 1: Using Git (Recommended - Full Repository) ğŸš€
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
1. **Clone the Repository**:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   .. code-block:: bash

     git clone https://github.com/sohaibdaoudi/ChangingTireAssistant_CV_NLP_Project.git

2. **Navigate to the Object Detection Directory**: Open your terminal or command prompt and change to the `Object Detection` folder within the extracted contents.

Option 2: Manual Download (ZIP) â¬‡ï¸
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
1. **Download the Repository**:
   - **Full Repository**: Navigate to the `main repository page <https://github.com/sohaibdaoudi/ChangingTireAssistant_CV_NLP_Project>`_ and click on "Code" â†’ "Download ZIP".
   - **Object Detection**: A direct download for only the Object Detection folder is typically achieved by downloading the full repository and then extracting the relevant folder.

2. **Extract the ZIP File**: Unzip the downloaded file to your desired location.
3. **Navigate to the Object Detection Directory**: Open your terminal or command prompt and change to the `Object Detection` folder within the extracted contents.

---

Installation and Execution ğŸš€
==============================
0. Navigate to Your Project Directory ğŸ“‚
------------------------------------------
First, open your command line interface (e.g., Command Prompt, PowerShell, Terminal, Anaconda Prompt) and navigate to the project's root folder. Replace `C:\path_to_folder` with the actual path to your Object Detection folder.


.. code-block:: bash

    cd C:\path_to_folder

1. Create a Virtual Environment (Recommended) (Choose **one** of the following methods) ğŸ:
-----------------------------------------------------------------------------------------------------
   * **Using `venv` (Python's built-in)**:

  
     .. code-block:: bash

       python -m venv venv

     Activate the environment:

     * On Windows:

       .. code-block:: bash

         venv\Scripts\activate

     * On macOS/Linux:


       .. code-block:: bash

         source venv/bin/activate

   * **Using `conda` (Anaconda/Miniconda)**:
     Replace `myenv` with your desired environment name and your preferred compatible version of python (3.1X).


     .. code-block:: bash
     

       conda create --name myenv python

     Activate the Conda environment:

     .. code-block:: bash
     

       conda activate myenv

2. Install Dependencies ğŸ“¦
---------------------------------------------------------------------
Ensure your `pip` is up to date (if using `venv` or pip within Conda) and then install the required packages.



.. code-block:: bash

    python -m pip install --upgrade pip
    pip install ultralytics==XX.XX.XX opencv-python==XX.XX.XX

*If using Conda, you might prefer to install packages using Conda where possible, for example:*



.. code-block:: bash

    # conda install anaconda # For a fuller anaconda distribution within the environment if needed
    # conda install pip # To ensure pip is available in the conda env
    # pip install ultralytics==XX.XX.XX opencv-python==XX.XX.XX

3. Run Detection â–¶ï¸
------------------------------------------------------------------------------------
Execute the detection script. The following command runs the detection on the test video using the GPU (or using only CPU by removing --device 0).



.. code-block:: bash


    python yolo_detect.py --model best.pt --source test.mp4 --resolution 1280x720 --device 0

* ``--model best.pt``: Specifies the path to your trained model weights.
* ``--source test.mp4``: Specifies the path to your input video or image source.
* ``--resolution 1280x720``: Sets the input resolution. This flag supports various input sizes.
* ``--device 0``: This command will let the test run on GPU, you can delete it if you want to use only CPU.

---

Action Recognition ğŸƒ
============================
*(Details for the Action Recognition module, including specific models, requirements, setup, installation, and execution, should be added here following a similar structure to the Object Detection section.)*

---

Authors ğŸ‘¥
==============
This project is developed and maintained by:

* **SOHAIB DAOUDI** â€“ `soh.daoudi@gmail.com <mailto:soh.daoudi@gmail.com>`_
* **MAROUANE MAJIDI** â€“ `majidi.marouane0@gmail.com <mailto:majidi.marouane0@gmail.com>`_

---

License ğŸ“„
=============
This project is licensed under the `MIT License <https://opensource.org/licenses/MIT>`_. Please see the ``LICENSE`` file in the repository for full license text and details.