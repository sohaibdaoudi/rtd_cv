

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>III-Object Detection &mdash; Changing flat tyre assistant  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="IV- Voice Assistant" href="5_Assistant_chatbot.html" />
    <link rel="prev" title="III. Action Recognition" href="3_Action_recognition.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Changing flat tyre assistant
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Table des matières:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="1_Introduction.html">I-Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_Team.html">II. Team</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_Action_recognition.html">III. Action Recognition</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">III-Object Detection</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#data-collection">3.1 Data Collection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-sources">3.1.1 Data Sources</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-labeling">3.1.2 Data Labeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-augmentation">3.1.3 Data Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#video-frame-extraction">3.1.4 Video Frame Extraction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-selection-and-training">3.2 Model Selection and Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#architecture-evaluation-yolov11-variants">3.2.1 Architecture Evaluation: YOLOv11 Variants</a></li>
<li class="toctree-l3"><a class="reference internal" href="#comparative-analysis-yolov11n-vs-yolov8n">3.2.2 Comparative Analysis: YOLOv11n vs. YOLOv8n</a></li>
<li class="toctree-l3"><a class="reference internal" href="#final-model-strategy">3.2.3 Final Model Strategy</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="5_Assistant_chatbot.html">IV- Voice Assistant</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_Algorithme.html">VI – Assistant Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_Installation_And_Usage.html">VII - Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="8_perspectives.html">VIII – Future Enhancements</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Changing flat tyre assistant</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">III-Object Detection</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/Documentation/Scripts/4_Object_detection.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="iii-object-detection">
<h1>III-Object Detection<a class="headerlink" href="#iii-object-detection" title="Link to this heading"></a></h1>
<p>In our project, Object Detection represents the foundational stage of a comprehensive, eleven-step guided assistance system. The first two steps of our system rely exclusively on object detection models to assess the situation and ensure the necessary equipment is available.</p>
<p>Specifically, one model is trained to detect the problem state (a “flat tire”), while a second model is trained to identify the required tools (“Car_jack” and “Wheel_wrench”). The successful detection of these tools is the critical prerequisite for initiating the main operational workflow. If the tools are not present, the system cannot proceed to the subsequent phases (Steps 3 through 11), which are handled by an action recognition model detailed later in this report. This chapter details the entire workflow for developing these essential object detection models, from data acquisition and preparation to model selection, training, and evaluation.</p>
<section id="data-collection">
<h2>3.1 Data Collection<a class="headerlink" href="#data-collection" title="Link to this heading"></a></h2>
<p>The foundation of any successful machine learning model is a high-quality, comprehensive dataset. For this project, we assembled a custom dataset of images to train our object detection model. The data collection process was twofold, involving both web scraping and manual extraction of frames from videos.</p>
<section id="data-sources">
<h3>3.1.1 Data Sources<a class="headerlink" href="#data-sources" title="Link to this heading"></a></h3>
<p>Our dataset comprises a total of 1805 images, categorized into two main classes: “flat tire” and “tools.” The tools class is further subdivided into “Wheel_wrench” and “Car_jack.”</p>
<ul class="simple">
<li><p><strong>Flat Tire Images:</strong> We collected 352 images of flat tires by scraping various search engines, including Google, Bing, and Brave. This approach provided a diverse set of images with different lighting conditions, angles, and backgrounds.</p></li>
<li><p><strong>Tool Images:</strong> The tool dataset consists of 1453 images. Of these, 605 were obtained through web scraping, and the remaining 848 were created by extracting frames from videos we recorded. This dual-sourcing strategy ensured a rich and varied dataset, capturing the tools in different contexts and orientations.</p></li>
</ul>
</section>
<section id="data-labeling">
<h3>3.1.2 Data Labeling<a class="headerlink" href="#data-labeling" title="Link to this heading"></a></h3>
<p>All 1805 images were manually labeled using the <cite>LabelImg</cite> annotation tool. This process involved drawing bounding boxes around each object of interest and assigning the appropriate class label. The distribution of labels is as follows:</p>
<ul class="simple">
<li><p><strong>Flat_tire (ID: 0):</strong> 357 labels</p></li>
</ul>
<p><strong>Example of scrapped data of flat tire:</strong></p>
<a class="reference internal image-reference" href="../../_images/collected_flat.png"><img alt="Example of scrapped data of flat tire" class="align-center" src="../../_images/collected_flat.png" style="width: 400px;" />
</a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<ul class="simple">
<li><p><strong>Car_Jack (ID: 0):</strong> 930 labels</p></li>
<li><p><strong>Wheel_Wrench (ID: 1):</strong> 1759 labels</p></li>
</ul>
<p>The higher number of labels for the <cite>Wheel_Wrench</cite> class is due to the inclusion of two distinct types of wrenches: the 4-way and L-shaped models.</p>
<table class="nocolor docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>Example of scrapped data of tools:</strong></p>
<a class="reference internal image-reference" href="../../_images/collected_data.png"><img alt="Example of scrapped data of tools" class="align-center" src="../../_images/collected_data.png" style="width: 400px;" />
</a>
</td>
<td><p><strong>Example of created data of tools:</strong></p>
<a class="reference internal image-reference" href="../../_images/created_data.png"><img alt="Example of created data of tools" class="align-center" src="../../_images/created_data.png" style="width: 400px;" />
</a>
</td>
</tr>
</tbody>
</table>
</section>
<section id="data-augmentation">
<h3>3.1.3 Data Augmentation<a class="headerlink" href="#data-augmentation" title="Link to this heading"></a></h3>
<p>To enhance the diversity of our dataset and improve the model’s ability to generalize, we applied data augmentation techniques. A portion of the images was converted to grayscale. This helps the model to be less sensitive to color variations and focus more on the shapes and textures of the objects.</p>
</section>
<section id="video-frame-extraction">
<h3>3.1.4 Video Frame Extraction<a class="headerlink" href="#video-frame-extraction" title="Link to this heading"></a></h3>
<p>To create a more realistic and varied dataset for the “tools” class, we extracted frames from videos. This was accomplished using a Python script with the OpenCV library. The script reads a video file, and at a set interval of one second, it extracts a frame and saves it as a JPEG image. This method allowed us to generate a large number of unique images from a relatively small number of video files.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">cv2</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cv</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="c1"># Path to the video file</span>
<span class="n">video_path</span> <span class="o">=</span> <span class="s1">&#39;path/to/your/video.mp4&#39;</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="n">video_path</span><span class="p">)</span>

<span class="c1"># Get frames per second (fps) of the video</span>
<span class="n">fps</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">CAP_PROP_FPS</span><span class="p">)</span>
<span class="n">interval</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Interval in seconds</span>

<span class="c1"># Directory to save the extracted frames</span>
<span class="n">output_dir</span> <span class="o">=</span> <span class="s1">&#39;path/to/save/frames&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">frame_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">cap</span><span class="o">.</span><span class="n">isOpened</span><span class="p">():</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Can&#39;t receive frame (stream end?). Exiting ...&quot;</span><span class="p">)</span>
        <span class="k">break</span>

    <span class="n">current_time</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">CAP_PROP_POS_MSEC</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1000</span>  <span class="c1"># Current time in seconds</span>

    <span class="c1"># Save a frame every &#39;interval&#39; seconds</span>
    <span class="k">if</span> <span class="n">current_time</span> <span class="o">%</span> <span class="n">interval</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">fps</span><span class="p">):</span>
        <span class="n">frame_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;frame_</span><span class="si">{</span><span class="n">frame_count</span><span class="si">}</span><span class="s1">.jpg&#39;</span><span class="p">)</span>
        <span class="n">cv</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="n">frame_filename</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>
        <span class="n">frame_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">cv</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;q&#39;</span><span class="p">):</span>
        <span class="k">break</span>

<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">frame_count</span><span class="si">}</span><span class="s2"> frames saved.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Example video that was used to extract frames:</strong></p>
<img alt="Example video that was used to extract frames :width: 400px" class="align-center" src="../../_images/extract_frame.gif" />
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
<section id="model-selection-and-training">
<h2>3.2 Model Selection and Training<a class="headerlink" href="#model-selection-and-training" title="Link to this heading"></a></h2>
<p>Selecting the right model architecture is a critical trade-off between accuracy and computational efficiency. Our primary goal is to deploy the model on an Android application, which necessitates a lightweight model capable of real-time performance on resource-constrained devices.</p>
<section id="architecture-evaluation-yolov11-variants">
<h3>3.2.1 Architecture Evaluation: YOLOv11 Variants<a class="headerlink" href="#architecture-evaluation-yolov11-variants" title="Link to this heading"></a></h3>
<p>We experimented with several versions of the YOLOv11 architecture: Nano (n), Small (s), Medium (m), Large (l), and Extra-Large (x). Our training efforts for the YOLOv11x model were halted prematurely as the Kaggle session crashed due to insufficient GPU memory, highlighting the significant resource demands of larger models.</p>
<p>Our performance testing revealed a clear trend: as the model size increased from ‘n’ to ‘l’, accuracy improved, but at the cost of computational speed. Real-time video processing performance was as follows:</p>
<ul class="simple">
<li><p><strong>YOLOv11l:</strong> 8 FPS</p></li>
<li><p><strong>YOLOv11m:</strong> 15 FPS</p></li>
<li><p><strong>YOLOv11s:</strong> 20 FPS</p></li>
<li><p><strong>YOLOv11n (Nano):</strong> 30 FPS</p></li>
</ul>
<p>The YOLOv11n model consistently achieved 30 FPS, and by slightly reducing the camera resolution, we could further boost its performance to a stable 30-40 FPS. Given its superior speed and low computational footprint, <strong>YOLOv11n</strong> was selected as the optimal architecture for our mobile deployment.</p>
</section>
<section id="comparative-analysis-yolov11n-vs-yolov8n">
<h3>3.2.2 Comparative Analysis: YOLOv11n vs. YOLOv8n<a class="headerlink" href="#comparative-analysis-yolov11n-vs-yolov8n" title="Link to this heading"></a></h3>
<p>After settling on the “nano” variant, we conducted a comparative analysis between our chosen YOLOv11n and its predecessor, YOLOv8n, to validate our choice. YOLOv11 offers several architectural advancements over YOLOv8, including:</p>
<ul class="simple">
<li><p><strong>Enhanced Feature Extraction:</strong> An improved backbone and neck design for more precise object localization.</p></li>
<li><p><strong>Optimized Efficiency:</strong> Faster processing speeds while maintaining high accuracy.</p></li>
<li><p><strong>Greater Accuracy with Fewer Parameters:</strong> YOLOv11 achieves a higher mean Average Precision (mAP) with a more compact model size.</p></li>
<li><p><strong>Broad Task Support:</strong> Capable of handling a wide range of computer vision tasks beyond simple object detection.</p></li>
<li><p><strong>Adaptability:</strong> Designed to perform efficiently across diverse environments, from edge devices to cloud GPUs.</p></li>
</ul>
<p>The training results for the “tools” model bear out these advantages.</p>
<p><strong>YOLOv11n Training Results:</strong></p>
<img alt="YOLOv11n training summary" src="../../_images/yolov11n_results.png" />
<ul class="simple">
<li><p><strong>Layers:</strong> 100</p></li>
<li><p><strong>Parameters:</strong> 2,582,542</p></li>
<li><p><strong>mAP50:</strong> 0.972</p></li>
<li><p><strong>mAP50-95:</strong> 0.862</p></li>
</ul>
<p><strong>YOLOv8n Training Results:</strong></p>
<img alt="YOLOv8n training summary" src="../../_images/yolov8n_results.png" />
<ul class="simple">
<li><p><strong>Layers:</strong> 72</p></li>
<li><p><strong>Parameters:</strong> 3,006,038</p></li>
<li><p><strong>mAP50:</strong> 0.975</p></li>
<li><p><strong>mAP50-95:</strong> 0.845</p></li>
</ul>
<p>While YOLOv8n has a slightly higher mAP at the 0.50 IoU threshold (0.975 vs 0.972), YOLOv11n achieves a better mAP across all thresholds (0.862 vs 0.845) with nearly half a million fewer parameters. This demonstrates that YOLOv11n is a more efficient and accurate model for this specific task, making it the definitive choice for a project requiring high accuracy with optimized computational performance.</p>
</section>
<section id="final-model-strategy">
<h3>3.2.3 Final Model Strategy<a class="headerlink" href="#final-model-strategy" title="Link to this heading"></a></h3>
<p>To maximize accuracy and prevent task confusion, we implemented a two-model strategy. Instead of a single, multi-class model, we trained two separate, highly specialized <strong>YOLOv11n</strong> models:</p>
<ol class="arabic simple">
<li><p><strong>Flat Tire Detection Model:</strong> Exclusively trained on images of flat tires.</p></li>
<li><p><strong>Tool Detection Model:</strong> Exclusively trained on images of the <cite>Car_Jack</cite> and <cite>Wheel_Wrench</cite>.</p></li>
</ol>
<p>This separation ensures that the tool detection model does not erroneously identify a flat tire during the tool-gathering phase, and vice-versa. This focused approach leads to a more robust and reliable system, which is paramount for our guided assistance application.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="3_Action_recognition.html" class="btn btn-neutral float-left" title="III. Action Recognition" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="5_Assistant_chatbot.html" class="btn btn-neutral float-right" title="IV- Voice Assistant" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Changing flat tyre assistant.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>